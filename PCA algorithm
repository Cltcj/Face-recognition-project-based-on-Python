# development time;0019 nothing 2021/4/19
#PCA algorithm (Principal Component Analysis)
PCA stands for Principal Component Analysis (PCA), which is a linear dimensionality reduction algorithm
#1. The dataset is {x1,x2...If the dimension is m, we can write the data set as the matrix X(m*n) with m rows and n columns.
#2. If you want to reduce its dimension k, then:
#3. Subtract the average value of each feature from each data and each bit feature (each row) of the data set, that is, zero mean is carried out for each feature dimension in the data to get the normalized matrix B
C=1/n*B*B.T
#5. Decompose eigenvalues and eigenvectors of covariance matrix C
#6. Sort the decomposed eigenvalues from large to small, select the largest k of them, and then use k eigenvectors corresponding to these eigenvalues as row vectors to form the eigenvector matrix P
#7. Get the result after dimensionality reduction: Y=PX

import numpy as np

# Data center (zero mean) processing
def Centralization_function(data):
    '''
    :param data:The data for {x1, x2...Xn}, if the dimension is m, we can write the data set as the matrix A(m*n) with m rows and n columns.
    :return: Data - The average of the corresponding vectors in data
    '''
    zero_mean_matrix=np.nanmean(data,axis=0)
    #print(data-zero_mean_matrix)
    return data-zero_mean_matrix
    
    
    
#Based on conventional eigenvalue decomposition
def pca_conventional_eigenvalue(data,n):
    '''
    :param data: Initial data set
    :param n:dimension
      :return:Returns the result of dimensionality reduction, Y=PX
    '''
    new_data=Centralization_function(data)#data-zero_mean_matrix
    C_mat=1/n*np.dot(new_data.T,new_data)#covariance matrix :C=1/n*B*B.T
    #print(cov_mat)
    eigenvalue_values,eigenvalue_vectors=np.linalg.eig(np.mat(C_mat))#Eigenvalues and eigenvectors in C
    #print(eigenvalue_values)
    #print(eigenvalue_vectors)
    value_indices=np.argsort(eigenvalue_values)#sort from smallest to biggest

    #n_vectors=eigenvalue_vectors[:,value_indices[-1:-(n+1):-1]]# The eigenvector corresponding to the maximum n eigenvalues is copied from the last element to the first element
    n_vectors=eigenvalue_vectors[:,value_indices[::-1]]
    #print(n_vectors)
    return new_data*n_vectors

def pca_svd(data,k):
    new_data=Centralization_function(data)
    Cov_mat=1/k*np.dot(new_data.T,new_data)
    U,s,V=np.linalg.svd(Cov_mat)
    pc=np.dot(new_data,U)
    return pc[:,0]
def test():
    data=np.array(
        [[2.5,2.4],[0.5,0.7],[2.2,2.9],[1.9,2.2],[3.1,3.0],[2.3,2.7],[2,1.6],[1,1.1],[1.5,1.6],[1.1,0.9]]
    )
    result_eig=pca_svd(data,1)
    print(result_eig)

if __name__=='__main__':
    test()

